{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.linalg import svd\n",
    "from typing import List, Tuple\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet, brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Corpora\n",
    "\n",
    "I am using three short pieces by Charles S. Peirce as a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"texts/\" + file_name for file_name in (\"fixation_of_belief.txt\",\n",
    "                                                     \"how_to_make_our_ideas_clear.txt\",\n",
    "                                                     \"the_doctrine_of_chances.txt\",\n",
    "                                                     \"the_probability_of_induction.txt\")]\n",
    "corpus = \"\"\n",
    "\n",
    "for path in file_paths:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        corpus += file.read()\n",
    "        \n",
    "corpus = corpus.replace(\"—\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First two sentences from Peirce's _The Fixation of Belief_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Few persons care to study logic, because everybody conceives himself to be proficient enough in the art of reasoning already. But I observe that this satisfaction is limited to one's own ratiocination, and does not extend to that of other men.\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:243]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Corpus\n",
    "\n",
    "The corpus is tokenized, lemmatized, and a vocabulary is created for it. This vocabulary does not include stopwords or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"stopwords\")  # might be required if corpus is not already downloaded, same for other corpora\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = stop_words.union(set(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict= {\n",
    "    \"J\": wordnet.ADJ,\n",
    "    \"V\": wordnet.VERB,\n",
    "    \"N\": wordnet.NOUN,\n",
    "    \"R\": wordnet.ADV\n",
    "}\n",
    "\n",
    "\n",
    "def lemmatize_corpus(tokenized_corpus: List[str]) -> List[str]:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_corpus = nltk.pos_tag(tokenized_corpus)\n",
    "    \n",
    "    lemmatized_corpus = [lemmatizer.lemmatize(word, tag_dict.get(tag, wordnet.NOUN)).lower() for word, tag in tagged_corpus]\n",
    "    return lemmatized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(corpus: str) -> Tuple[List[str], List[str]]:\n",
    "    tokenized_corpus = word_tokenize(corpus)\n",
    "    lemmatized_corpus = lemmatize_corpus(tokenized_corpus)\n",
    "    \n",
    "    vocab = list(set(lemmatized_corpus).difference(stop_words))\n",
    "    \n",
    "    return lemmatized_corpus, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Count Matrix\n",
    "\n",
    "The core function `generate_count_matrix` creates a count matrix and a vocabulary index for that matrix. The implementation here is general for all sizes of sliding windows, but they need to be symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_count_matrix(processed_corpus: List[str], vocab: List[str], n: int = 2) -> Tuple[np.matrix, dict]:\n",
    "    assert n > 0, \"Do not use negative sizes for sliding windows\"\n",
    "    \n",
    "    vocab_index = {token: i for i, token in enumerate(vocab)}\n",
    "    count_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    \n",
    "    for i, token in enumerate(processed_corpus):\n",
    "        if token not in vocab:\n",
    "                continue\n",
    "                \n",
    "        index = vocab_index[token]\n",
    "        \n",
    "        surrounding_indices =  [j for j in range(i-n, i) if j >= 0] + [j for j in range(i+1, i+n+1) if j < len(processed_corpus)]\n",
    "        for j in surrounding_indices:\n",
    "            other_token = processed_corpus[j]\n",
    "            \n",
    "            if other_token not in vocab:\n",
    "                continue\n",
    "            \n",
    "            other_index = vocab_index[other_token]\n",
    "            count_matrix[index][other_index] += 1\n",
    "            count_matrix[other_index][index] += 1\n",
    "    \n",
    "    return np.matrix(count_matrix), vocab_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Matrix for Peirce's Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accord</th>\n",
       "      <th>advancing</th>\n",
       "      <th>ashamed</th>\n",
       "      <th>necessity</th>\n",
       "      <th>guild</th>\n",
       "      <th>thought</th>\n",
       "      <th>die</th>\n",
       "      <th>is—</th>\n",
       "      <th>directly</th>\n",
       "      <th>contained</th>\n",
       "      <th>...</th>\n",
       "      <th>image</th>\n",
       "      <th>horror</th>\n",
       "      <th>metaphysical</th>\n",
       "      <th>remember</th>\n",
       "      <th>imperceptible</th>\n",
       "      <th>great</th>\n",
       "      <th>lapse</th>\n",
       "      <th>company</th>\n",
       "      <th>be—with</th>\n",
       "      <th>vanishes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advancing</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ashamed</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>necessity</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guild</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lapse</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be—with</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vanishes</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3148 rows × 3148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           accord  advancing  ashamed  necessity  guild  thought  die  is—  \\\n",
       "accord        0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "advancing     0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "ashamed       0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "necessity     0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "guild         0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "...           ...        ...      ...        ...    ...      ...  ...  ...   \n",
       "great         0.0        0.0      0.0        0.0    0.0      2.0  0.0  0.0   \n",
       "lapse         0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "company       0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "be—with       0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "vanishes      0.0        0.0      0.0        0.0    0.0      0.0  0.0  0.0   \n",
       "\n",
       "           directly  contained  ...  image  horror  metaphysical  remember  \\\n",
       "accord          0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "advancing       0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "ashamed         0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "necessity       0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "guild           0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "...             ...        ...  ...    ...     ...           ...       ...   \n",
       "great           0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "lapse           0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "company         0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "be—with         0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "vanishes        0.0        0.0  ...    0.0     0.0           0.0       0.0   \n",
       "\n",
       "           imperceptible  great  lapse  company  be—with  vanishes  \n",
       "accord               0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "advancing            0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "ashamed              0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "necessity            0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "guild                0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "...                  ...    ...    ...      ...      ...       ...  \n",
       "great                0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "lapse                0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "company              0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "be—with              0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "vanishes             0.0    0.0    0.0      0.0      0.0       0.0  \n",
       "\n",
       "[3148 rows x 3148 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_corpus, vocab = prepare_corpus(corpus)\n",
    "count_matrix, vocab_index = generate_count_matrix(prepared_corpus, vocab, 5)\n",
    "pd.DataFrame(count_matrix, index=vocab_index, columns=vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen here the initial count matrix is extremely sparse. This sparseness results from a relatively large vocabulary for a small corpus."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this cell can be converted to code to create a count matrix for the brown corpus. It takes quite a while to run, however.\n",
    "\n",
    "news_text = list(brown.words(categories=\"news\"))\n",
    "brown_corpus = lemmatize_corpus(news_text)\n",
    "brown_vocab = list(set(brown_corpus).difference(stop_words))\n",
    "brown_count_matrix, brown_vocab_index = generate_count_matrix(brown_corpus, brown_vocab)\n",
    "pd.DataFrame(brown_count_matrix, index=brown_vocab_index, columns=brown_vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(first_vector: np.array, second_vector: np.array) -> float:\n",
    "    return 1 - distance.cosine(first_vector, second_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word: str, count_matrix: np.matrix, vocab_index: dict) -> np.array:\n",
    "    i = vocab_index[word]\n",
    "    return count_matrix[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_vector, h_vector, m_vector = map(lambda w: get_vector(w, count_matrix, vocab_index), (\"german\", \"horror\", \"metaphysical\"))\n",
    "\n",
    "cosine_similarity(g_vector, h_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(g_vector, m_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(h_vector, m_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Pointwise Mutual Information\n",
    "\n",
    "Mere counts lead to extremely sparse matrices and do not account for the differences in frequency between words at all. A relatively simple approach to address these issues is to use *Pointwise Mutual Information* (PMI). For the occurence of two words in the same window, PMI is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{log}(\\frac{\\text{P}(A,B)}{\\text{P}(A)*\\text{P}(B)})\n",
    "\\end{equation}\n",
    "\n",
    "We estimate the probabilities using frequency counts from the count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi_calculation(count_matrix: np.matrix, smoothing: float = 0.1) -> np.matrix:\n",
    "    count_array = np.array(count_matrix) + smoothing\n",
    "    \n",
    "    num_ngrams = count_array.sum() / 2\n",
    "    all_sums = np.sum(count_array, axis=0)\n",
    "    \n",
    "    pmi_array = np.zeros_like(count_array)\n",
    "    \n",
    "    for i1, i2 in zip(*np.triu_indices_from(count_array)):\n",
    "        p_ab = count_array[i1, i2] / num_ngrams\n",
    "        p_a = all_sums[i1] / num_ngrams\n",
    "        p_b = all_sums[i2] / num_ngrams\n",
    "                \n",
    "        pmi_array[i1, i2] = np.log(p_ab/(p_a*p_b))\n",
    "        \n",
    "    return np.matrix(np.where(pmi_array,pmi_array,pmi_array.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.76134055, -0.70740297, -0.68875731, ..., -0.7377267 ,\n",
       "         -0.70740297, -0.69501124],\n",
       "        [-0.70740297, -0.65346539, -0.63481973, ..., -0.68378912,\n",
       "         -0.65346539, -0.64107366],\n",
       "        [-0.68875731, -0.63481973, -0.61617407, ..., -0.66514346,\n",
       "         -0.63481973, -0.622428  ],\n",
       "        ...,\n",
       "        [-0.7377267 , -0.68378912, -0.66514346, ..., -0.71411285,\n",
       "         -0.68378912, -0.67139739],\n",
       "        [-0.70740297, -0.65346539, -0.63481973, ..., -0.68378912,\n",
       "         -0.65346539, -0.64107366],\n",
       "        [-0.69501124, -0.64107366, -0.622428  , ..., -0.67139739,\n",
       "         -0.64107366, -0.62868193]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_matrix = pmi_calculation(count_matrix)\n",
    "p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9749159487499798"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_vector, h_vector, m_vector = map(lambda w: get_vector(w, p_matrix, vocab_index), (\"german\", \"horror\", \"metaphysical\"))\n",
    "\n",
    "cosine_similarity(g_vector, h_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9454539776417508"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(g_vector, m_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9479503050921366"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(h_vector, m_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_reduction(matrix: np.matrix, n_components: int = 100) -> np.matrix:\n",
    "    u, s, vh = svd(matrix,\n",
    "                   full_matrices=False,\n",
    "                   compute_uv=True)\n",
    "    \n",
    "    u = np.array(u) # multiplication will fail if u is a matrix, because the axes are not appropriately reduced\n",
    "\n",
    "    reduced_u = u[:, :n_components]\n",
    "    reduced_s = s[:n_components]\n",
    "    \n",
    "    return np.matrix(reduced_u * reduced_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3148, 100)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_matrix = svd_reduction(p_matrix)\n",
    "reduced_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>-42.692485</td>\n",
       "      <td>-0.865796</td>\n",
       "      <td>-0.675314</td>\n",
       "      <td>1.834893</td>\n",
       "      <td>-0.125814</td>\n",
       "      <td>1.620761</td>\n",
       "      <td>-0.911219</td>\n",
       "      <td>0.220854</td>\n",
       "      <td>0.152028</td>\n",
       "      <td>0.100691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192854</td>\n",
       "      <td>0.140407</td>\n",
       "      <td>-0.122672</td>\n",
       "      <td>0.341055</td>\n",
       "      <td>-0.326309</td>\n",
       "      <td>0.615228</td>\n",
       "      <td>-0.744698</td>\n",
       "      <td>-0.368974</td>\n",
       "      <td>-0.224093</td>\n",
       "      <td>0.038963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advancing</th>\n",
       "      <td>-40.384351</td>\n",
       "      <td>2.466395</td>\n",
       "      <td>-0.001764</td>\n",
       "      <td>0.039351</td>\n",
       "      <td>0.109633</td>\n",
       "      <td>0.081963</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.068598</td>\n",
       "      <td>-0.109821</td>\n",
       "      <td>0.023931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011790</td>\n",
       "      <td>-0.032162</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>0.052126</td>\n",
       "      <td>0.211922</td>\n",
       "      <td>-0.130667</td>\n",
       "      <td>-0.076479</td>\n",
       "      <td>0.039511</td>\n",
       "      <td>-0.013310</td>\n",
       "      <td>0.039389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ashamed</th>\n",
       "      <td>-39.462359</td>\n",
       "      <td>2.196982</td>\n",
       "      <td>0.086688</td>\n",
       "      <td>-0.046295</td>\n",
       "      <td>0.344482</td>\n",
       "      <td>-0.036429</td>\n",
       "      <td>0.157410</td>\n",
       "      <td>-0.134625</td>\n",
       "      <td>-0.165391</td>\n",
       "      <td>-0.302655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355941</td>\n",
       "      <td>0.188126</td>\n",
       "      <td>-0.166463</td>\n",
       "      <td>-0.211797</td>\n",
       "      <td>0.152439</td>\n",
       "      <td>0.279711</td>\n",
       "      <td>0.079367</td>\n",
       "      <td>0.228621</td>\n",
       "      <td>0.024152</td>\n",
       "      <td>0.291290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>necessity</th>\n",
       "      <td>-42.441759</td>\n",
       "      <td>0.788009</td>\n",
       "      <td>-0.490228</td>\n",
       "      <td>0.252850</td>\n",
       "      <td>0.191708</td>\n",
       "      <td>0.148791</td>\n",
       "      <td>0.858232</td>\n",
       "      <td>-0.078095</td>\n",
       "      <td>-0.576744</td>\n",
       "      <td>0.383605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407634</td>\n",
       "      <td>-0.905238</td>\n",
       "      <td>-0.190688</td>\n",
       "      <td>0.546488</td>\n",
       "      <td>0.283897</td>\n",
       "      <td>0.534433</td>\n",
       "      <td>-0.019515</td>\n",
       "      <td>-0.286762</td>\n",
       "      <td>0.263737</td>\n",
       "      <td>0.201086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guild</th>\n",
       "      <td>-39.512785</td>\n",
       "      <td>2.586151</td>\n",
       "      <td>-0.063118</td>\n",
       "      <td>0.055959</td>\n",
       "      <td>-0.007730</td>\n",
       "      <td>0.054701</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.100811</td>\n",
       "      <td>-0.173149</td>\n",
       "      <td>-0.028282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>-0.042799</td>\n",
       "      <td>-0.024174</td>\n",
       "      <td>-0.004617</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>0.047248</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.040719</td>\n",
       "      <td>-0.012966</td>\n",
       "      <td>-0.044896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>-68.078796</td>\n",
       "      <td>-13.604252</td>\n",
       "      <td>2.467828</td>\n",
       "      <td>-3.111068</td>\n",
       "      <td>3.123450</td>\n",
       "      <td>-6.019949</td>\n",
       "      <td>3.810469</td>\n",
       "      <td>-3.315964</td>\n",
       "      <td>0.184549</td>\n",
       "      <td>0.799527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265622</td>\n",
       "      <td>-1.461481</td>\n",
       "      <td>4.298493</td>\n",
       "      <td>-0.834530</td>\n",
       "      <td>-1.224188</td>\n",
       "      <td>1.456485</td>\n",
       "      <td>-1.946560</td>\n",
       "      <td>1.945605</td>\n",
       "      <td>-2.591400</td>\n",
       "      <td>2.501025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lapse</th>\n",
       "      <td>-41.300278</td>\n",
       "      <td>0.389663</td>\n",
       "      <td>-0.033189</td>\n",
       "      <td>0.569073</td>\n",
       "      <td>-0.514934</td>\n",
       "      <td>-0.237593</td>\n",
       "      <td>0.631704</td>\n",
       "      <td>0.556434</td>\n",
       "      <td>-0.903813</td>\n",
       "      <td>-1.285619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404485</td>\n",
       "      <td>-0.116450</td>\n",
       "      <td>0.788800</td>\n",
       "      <td>0.399657</td>\n",
       "      <td>-0.440245</td>\n",
       "      <td>-0.049655</td>\n",
       "      <td>0.142707</td>\n",
       "      <td>0.547066</td>\n",
       "      <td>0.524710</td>\n",
       "      <td>0.207904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>-41.656822</td>\n",
       "      <td>1.323748</td>\n",
       "      <td>0.072601</td>\n",
       "      <td>-0.015611</td>\n",
       "      <td>-0.754509</td>\n",
       "      <td>0.071912</td>\n",
       "      <td>-0.403497</td>\n",
       "      <td>0.088802</td>\n",
       "      <td>-0.389782</td>\n",
       "      <td>0.023938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265604</td>\n",
       "      <td>0.243128</td>\n",
       "      <td>0.282918</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.497149</td>\n",
       "      <td>-0.168862</td>\n",
       "      <td>-0.270610</td>\n",
       "      <td>0.145967</td>\n",
       "      <td>-0.045790</td>\n",
       "      <td>-0.100996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be—with</th>\n",
       "      <td>-40.297824</td>\n",
       "      <td>1.783976</td>\n",
       "      <td>-0.107576</td>\n",
       "      <td>0.658190</td>\n",
       "      <td>-0.104900</td>\n",
       "      <td>0.679234</td>\n",
       "      <td>-0.306919</td>\n",
       "      <td>-0.158661</td>\n",
       "      <td>-0.129861</td>\n",
       "      <td>0.094472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.061639</td>\n",
       "      <td>0.020477</td>\n",
       "      <td>-0.075395</td>\n",
       "      <td>-0.198337</td>\n",
       "      <td>-0.083008</td>\n",
       "      <td>-0.096684</td>\n",
       "      <td>0.294888</td>\n",
       "      <td>-0.253449</td>\n",
       "      <td>-1.265558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vanishes</th>\n",
       "      <td>-39.735052</td>\n",
       "      <td>2.067205</td>\n",
       "      <td>0.071440</td>\n",
       "      <td>0.034807</td>\n",
       "      <td>0.028588</td>\n",
       "      <td>0.016217</td>\n",
       "      <td>0.452723</td>\n",
       "      <td>-0.465500</td>\n",
       "      <td>-0.376526</td>\n",
       "      <td>0.089066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.517458</td>\n",
       "      <td>0.274185</td>\n",
       "      <td>0.253369</td>\n",
       "      <td>0.436276</td>\n",
       "      <td>-0.044419</td>\n",
       "      <td>-0.327550</td>\n",
       "      <td>-0.394028</td>\n",
       "      <td>0.190969</td>\n",
       "      <td>-0.482853</td>\n",
       "      <td>-0.211681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3148 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0          1         2         3         4         5   \\\n",
       "accord    -42.692485  -0.865796 -0.675314  1.834893 -0.125814  1.620761   \n",
       "advancing -40.384351   2.466395 -0.001764  0.039351  0.109633  0.081963   \n",
       "ashamed   -39.462359   2.196982  0.086688 -0.046295  0.344482 -0.036429   \n",
       "necessity -42.441759   0.788009 -0.490228  0.252850  0.191708  0.148791   \n",
       "guild     -39.512785   2.586151 -0.063118  0.055959 -0.007730  0.054701   \n",
       "...              ...        ...       ...       ...       ...       ...   \n",
       "great     -68.078796 -13.604252  2.467828 -3.111068  3.123450 -6.019949   \n",
       "lapse     -41.300278   0.389663 -0.033189  0.569073 -0.514934 -0.237593   \n",
       "company   -41.656822   1.323748  0.072601 -0.015611 -0.754509  0.071912   \n",
       "be—with   -40.297824   1.783976 -0.107576  0.658190 -0.104900  0.679234   \n",
       "vanishes  -39.735052   2.067205  0.071440  0.034807  0.028588  0.016217   \n",
       "\n",
       "                 6         7         8         9   ...        90        91  \\\n",
       "accord    -0.911219  0.220854  0.152028  0.100691  ... -0.192854  0.140407   \n",
       "advancing -0.079531 -0.068598 -0.109821  0.023931  ... -0.011790 -0.032162   \n",
       "ashamed    0.157410 -0.134625 -0.165391 -0.302655  ...  0.355941  0.188126   \n",
       "necessity  0.858232 -0.078095 -0.576744  0.383605  ...  0.407634 -0.905238   \n",
       "guild      0.002498  0.100811 -0.173149 -0.028282  ...  0.048584 -0.042799   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "great      3.810469 -3.315964  0.184549  0.799527  ... -0.265622 -1.461481   \n",
       "lapse      0.631704  0.556434 -0.903813 -1.285619  ... -0.404485 -0.116450   \n",
       "company   -0.403497  0.088802 -0.389782  0.023938  ...  0.265604  0.243128   \n",
       "be—with   -0.306919 -0.158661 -0.129861  0.094472  ...  0.038217  0.061639   \n",
       "vanishes   0.452723 -0.465500 -0.376526  0.089066  ... -0.517458  0.274185   \n",
       "\n",
       "                 92        93        94        95        96        97  \\\n",
       "accord    -0.122672  0.341055 -0.326309  0.615228 -0.744698 -0.368974   \n",
       "advancing -0.000066  0.052126  0.211922 -0.130667 -0.076479  0.039511   \n",
       "ashamed   -0.166463 -0.211797  0.152439  0.279711  0.079367  0.228621   \n",
       "necessity -0.190688  0.546488  0.283897  0.534433 -0.019515 -0.286762   \n",
       "guild     -0.024174 -0.004617  0.012293  0.047248  0.003518  0.040719   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "great      4.298493 -0.834530 -1.224188  1.456485 -1.946560  1.945605   \n",
       "lapse      0.788800  0.399657 -0.440245 -0.049655  0.142707  0.547066   \n",
       "company    0.282918  0.013781  0.497149 -0.168862 -0.270610  0.145967   \n",
       "be—with    0.020477 -0.075395 -0.198337 -0.083008 -0.096684  0.294888   \n",
       "vanishes   0.253369  0.436276 -0.044419 -0.327550 -0.394028  0.190969   \n",
       "\n",
       "                 98        99  \n",
       "accord    -0.224093  0.038963  \n",
       "advancing -0.013310  0.039389  \n",
       "ashamed    0.024152  0.291290  \n",
       "necessity  0.263737  0.201086  \n",
       "guild     -0.012966 -0.044896  \n",
       "...             ...       ...  \n",
       "great     -2.591400  2.501025  \n",
       "lapse      0.524710  0.207904  \n",
       "company   -0.045790 -0.100996  \n",
       "be—with   -0.253449 -1.265558  \n",
       "vanishes  -0.482853 -0.211681  \n",
       "\n",
       "[3148 rows x 100 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reduced_matrix, index=vocab_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99433847303314"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_vector, h_vector, m_vector = map(lambda w: get_vector(w, reduced_matrix, vocab_index), (\"german\", \"horror\", \"metaphysical\"))\n",
    "\n",
    "cosine_similarity(g_vector, h_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9924418541350267"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(g_vector, m_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.990281134430607"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(h_vector, m_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
