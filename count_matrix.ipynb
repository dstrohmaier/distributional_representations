{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Basic Distributional Representations\n",
    "\n",
    "When I started my studies in NLP, Word2Vec embeddings were already the standard for representing word types. Distributional representations based on counting the occurence of words were mentioned as precursers of Word2Vec embeddings, but we never got much hands on experience with them. In this notebook I explore basic techniques of creating such distributional representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import svd\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet, brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Corpora\n",
    "\n",
    "I am using six short pieces by philosopher and logician Charles S. Peirce as my corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"texts/\" + file_name for file_name in (\"fixation_of_belief.txt\",\n",
    "                                                     \"how_to_make_our_ideas_clear.txt\",\n",
    "                                                     \"the_doctrine_of_chances.txt\",\n",
    "                                                     \"the_probability_of_induction.txt\",\n",
    "                                                     \"the_order_of_nature.txt\",\n",
    "                                                     \"deduction,_induction_,and_hypothesis.txt\")]\n",
    "corpus = \"\"\n",
    "\n",
    "for path in file_paths:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        corpus += file.read()\n",
    "        \n",
    "corpus = corpus.replace(\"—\", \" \")  # fix for tokenization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First two sentences from Peirce's _The Fixation of Belief_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Few persons care to study logic, because everybody conceives himself to be proficient enough in the art of reasoning already. But I observe that this satisfaction is limited to one's own ratiocination, and does not extend to that of other men.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:243]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Corpus\n",
    "\n",
    "To create distributional representations we have to prepare the corpus. The corpus is tokenized, lemmatized, and a vocabulary is created for it. This vocabulary does not include stopwords or punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I create a set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"stopwords\")  # might be required if corpus is not already downloaded, same for other corpora\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = stop_words.union(set(string.ascii_lowercase))\n",
    "stop_words = stop_words.union({\"doe\", \"'s\", \"ha\", \"wa\"})\n",
    "\n",
    "extended_punctuation = {\"''\", \"``\", \"´´\"}.union(set(string.punctuation))\n",
    "stop_words = stop_words.union(extended_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a function to lemmatize an already tokenized corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict= {\n",
    "    \"J\": wordnet.ADJ,\n",
    "    \"V\": wordnet.VERB,\n",
    "    \"N\": wordnet.NOUN,\n",
    "    \"R\": wordnet.ADV\n",
    "}\n",
    "\n",
    "\n",
    "def lemmatize_corpus(tokenized_corpus: List[str]) -> List[str]:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_corpus = nltk.pos_tag(tokenized_corpus)\n",
    "    \n",
    "    lemmatized_corpus = [lemmatizer.lemmatize(word, tag_dict.get(tag, wordnet.NOUN)).lower() for word, tag in tagged_corpus]\n",
    "    return lemmatized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a function that prepares an entire corpus given as a string. This function returns a lemmatized corpus and a vocabulary (without stopwords) for that corpus. `prepare_corpus` also allows to create a vocabulary only for the `n` most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(corpus: str, n: int = 0) -> Tuple[List[str], List[str]]:\n",
    "    tokenized_corpus = word_tokenize(corpus)\n",
    "    lemmatized_corpus = lemmatize_corpus(tokenized_corpus)\n",
    "    \n",
    "    if n == 0:\n",
    "        vocab = list(set(lemmatized_corpus).difference(stop_words))\n",
    "    else:\n",
    "        vocab_counter = Counter(lemmatized_corpus)\n",
    "        \n",
    "        for sw in stop_words:\n",
    "            vocab_counter[sw] = 0\n",
    "            \n",
    "        vocab = [w for w, _ in vocab_counter.most_common(n)]\n",
    "    \n",
    "    return lemmatized_corpus, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the Corpus\n",
    "\n",
    "Before diving into the distributional representation, it makes sense to explore the corpus a little. For example, we might want to check what the most common lemmas are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2869),\n",
       " ('the', 2670),\n",
       " ('of', 2110),\n",
       " ('.', 1452),\n",
       " ('a', 1424),\n",
       " ('to', 1227),\n",
       " ('is', 987),\n",
       " ('in', 866),\n",
       " ('that', 865),\n",
       " ('and', 832)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_corpus, vocab = prepare_corpus(corpus)\n",
    "counter = Counter(prepared_corpus)\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the 10 most common lemmas are stopwords, but we can also see the 10 most common content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 212),\n",
       " ('would', 201),\n",
       " ('may', 168),\n",
       " ('probability', 139),\n",
       " ('fact', 134),\n",
       " ('number', 133),\n",
       " ('belief', 117),\n",
       " ('character', 106),\n",
       " ('true', 105),\n",
       " ('case', 99)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sw in stop_words:\n",
    "    counter[sw] = 0\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5), dpi=100)\n",
    "\n",
    "labels, values = zip(*counter.most_common(25))\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.barh(indexes, values[::-1])\n",
    "plt.yticks(indexes, labels[::-1])\n",
    "\n",
    "plt.xlabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of all types in corpus (including stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4093"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of hapax legomena (words that occure once in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1938"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _, c in counter.items() if c == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Count Matrix\n",
    "\n",
    "The core function `generate_count_matrix` creates a count matrix and a vocabulary index for that matrix. The implementation here is general for all sizes of sliding windows, but they need to be symmetric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_count_matrix(processed_corpus: List[str], vocab: List[str], n: int = 2) -> Tuple[np.matrix, dict]:\n",
    "    assert n > 0, \"Do not use negative sizes for sliding windows\"\n",
    "    \n",
    "    vocab_index = {token: i for i, token in enumerate(vocab)}\n",
    "    count_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    \n",
    "    for i, token in enumerate(processed_corpus):\n",
    "        if token not in vocab:\n",
    "                continue\n",
    "                \n",
    "        index = vocab_index[token]\n",
    "        \n",
    "        prior_indices = [j for j in range(i-n, i) if j >= 0]\n",
    "        posterior_indices = [j for j in range(i+1, i+n+1) if j < len(processed_corpus)]\n",
    "        surrounding_indices =  prior_indices + posterior_indices\n",
    "        \n",
    "        for j in surrounding_indices:\n",
    "            other_token = processed_corpus[j]\n",
    "            \n",
    "            if other_token not in vocab:\n",
    "                continue\n",
    "            \n",
    "            other_index = vocab_index[other_token]\n",
    "            count_matrix[index][other_index] += 1\n",
    "            count_matrix[other_index][index] += 1\n",
    "    \n",
    "    return np.matrix(count_matrix), vocab_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Matrix for Peirce's Texts\n",
    "\n",
    "We can now create a simple co-occurence matrix for the corpus. Here I am taking a rather large sliding window of 5 on each side, i.e. each token sees its 10 surrounding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mentioned</th>\n",
       "      <th>impossibility</th>\n",
       "      <th>upon</th>\n",
       "      <th>inductively</th>\n",
       "      <th>untraveled</th>\n",
       "      <th>intent</th>\n",
       "      <th>ask</th>\n",
       "      <th>affirm</th>\n",
       "      <th>unfamiliar</th>\n",
       "      <th>un-b-ness</th>\n",
       "      <th>...</th>\n",
       "      <th>evolution</th>\n",
       "      <th>unrelated</th>\n",
       "      <th>wavering</th>\n",
       "      <th>denied</th>\n",
       "      <th>similarity</th>\n",
       "      <th>candour</th>\n",
       "      <th>refined</th>\n",
       "      <th>lightning</th>\n",
       "      <th>limited</th>\n",
       "      <th>exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mentioned</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impossibility</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upon</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inductively</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>untraveled</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candour</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refined</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightning</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>limited</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exist</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3857 rows × 3857 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               mentioned  impossibility  upon  inductively  untraveled  \\\n",
       "mentioned            0.0            0.0   0.0          0.0         0.0   \n",
       "impossibility        0.0            0.0   0.0          2.0         0.0   \n",
       "upon                 0.0            0.0   0.0          0.0         0.0   \n",
       "inductively          0.0            2.0   0.0          0.0         0.0   \n",
       "untraveled           0.0            0.0   0.0          0.0         0.0   \n",
       "...                  ...            ...   ...          ...         ...   \n",
       "candour              0.0            0.0   0.0          0.0         0.0   \n",
       "refined              0.0            0.0   0.0          0.0         0.0   \n",
       "lightning            0.0            0.0   2.0          0.0         0.0   \n",
       "limited              0.0            0.0   0.0          0.0         0.0   \n",
       "exist                0.0            0.0   0.0          0.0         0.0   \n",
       "\n",
       "               intent  ask  affirm  unfamiliar  un-b-ness  ...  evolution  \\\n",
       "mentioned         0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "impossibility     0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "upon              0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "inductively       0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "untraveled        0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "...               ...  ...     ...         ...        ...  ...        ...   \n",
       "candour           0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "refined           0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "lightning         0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "limited           0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "exist             0.0  0.0     0.0         0.0        0.0  ...        0.0   \n",
       "\n",
       "               unrelated  wavering  denied  similarity  candour  refined  \\\n",
       "mentioned            0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "impossibility        0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "upon                 0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "inductively          0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "untraveled           0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "...                  ...       ...     ...         ...      ...      ...   \n",
       "candour              0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "refined              0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "lightning            0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "limited              0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "exist                0.0       0.0     0.0         0.0      0.0      0.0   \n",
       "\n",
       "               lightning  limited  exist  \n",
       "mentioned            0.0      0.0    0.0  \n",
       "impossibility        0.0      0.0    0.0  \n",
       "upon                 2.0      0.0    0.0  \n",
       "inductively          0.0      0.0    0.0  \n",
       "untraveled           0.0      0.0    0.0  \n",
       "...                  ...      ...    ...  \n",
       "candour              0.0      0.0    0.0  \n",
       "refined              0.0      0.0    0.0  \n",
       "lightning            0.0      0.0    0.0  \n",
       "limited              0.0      8.0    0.0  \n",
       "exist                0.0      0.0    0.0  \n",
       "\n",
       "[3857 rows x 3857 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix, vocab_index = generate_count_matrix(prepared_corpus, vocab, 5)\n",
    "pd.DataFrame(count_matrix, index=vocab_index, columns=vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen here the initial count matrix is extremely sparse. This sparseness results from a relatively large vocabulary for a small corpus. While I picked a large window size most word types won't have tokens that meet in a window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a Small Count Matrix\n",
    "In this example I use only the 25 most common words to create a count matrix. As can be seen, this matrix is considerably denser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>would</th>\n",
       "      <th>may</th>\n",
       "      <th>probability</th>\n",
       "      <th>fact</th>\n",
       "      <th>number</th>\n",
       "      <th>belief</th>\n",
       "      <th>character</th>\n",
       "      <th>true</th>\n",
       "      <th>case</th>\n",
       "      <th>...</th>\n",
       "      <th>two</th>\n",
       "      <th>thing</th>\n",
       "      <th>method</th>\n",
       "      <th>time</th>\n",
       "      <th>different</th>\n",
       "      <th>another</th>\n",
       "      <th>induction</th>\n",
       "      <th>question</th>\n",
       "      <th>man</th>\n",
       "      <th>idea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>28.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probability</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fact</th>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>belief</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inference</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rule</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>must</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>certain</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thing</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>different</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>another</th>\n",
       "      <td>58.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>induction</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idea</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              one  would   may  probability  fact  number  belief  character  \\\n",
       "one           8.0   28.0  12.0          8.0  24.0     8.0     8.0       10.0   \n",
       "would        28.0    8.0   4.0          8.0  10.0    24.0     8.0        8.0   \n",
       "may          12.0    4.0   0.0          8.0   8.0     8.0    14.0       12.0   \n",
       "probability   8.0    8.0   8.0          8.0   8.0     6.0     2.0        4.0   \n",
       "fact         24.0   10.0   8.0          8.0  12.0     4.0     4.0        6.0   \n",
       "number        8.0   24.0   8.0          6.0   4.0    24.0     2.0        6.0   \n",
       "belief        8.0    8.0  14.0          2.0   4.0     2.0    16.0        0.0   \n",
       "character    10.0    8.0  12.0          4.0   6.0     6.0     0.0        8.0   \n",
       "true         12.0    8.0   4.0          8.0   6.0     4.0     8.0        4.0   \n",
       "case          0.0    6.0  14.0          8.0  10.0    10.0     0.0        4.0   \n",
       "inference    10.0    2.0  14.0         10.0   4.0     4.0     0.0        8.0   \n",
       "rule          2.0    6.0   8.0         12.0   6.0     2.0     8.0        0.0   \n",
       "first        10.0   12.0   2.0          6.0   4.0     4.0     0.0        4.0   \n",
       "must          4.0    2.0   0.0          2.0   6.0     2.0     2.0        4.0   \n",
       "certain       8.0    8.0   6.0          0.0   6.0     4.0     2.0       10.0   \n",
       "two          16.0    8.0   6.0         20.0   0.0    20.0     2.0        8.0   \n",
       "thing        10.0    2.0   4.0          4.0   2.0     6.0     2.0        8.0   \n",
       "method        4.0    4.0   6.0          2.0   4.0     0.0    12.0        0.0   \n",
       "time          6.0    4.0   4.0          0.0   0.0    14.0     0.0        0.0   \n",
       "different    12.0   10.0   8.0          6.0  16.0     6.0    12.0       12.0   \n",
       "another      58.0    8.0   4.0          8.0  22.0     2.0     2.0        6.0   \n",
       "induction     4.0    2.0   8.0          2.0   6.0     4.0     2.0        6.0   \n",
       "question      8.0    8.0   8.0          8.0  14.0     2.0     2.0        4.0   \n",
       "man          10.0   10.0   6.0          0.0   2.0     0.0     2.0        0.0   \n",
       "idea          6.0    0.0   4.0         10.0   6.0     2.0     2.0        0.0   \n",
       "\n",
       "             true  case  ...   two  thing  method  time  different  another  \\\n",
       "one          12.0   0.0  ...  16.0   10.0     4.0   6.0       12.0     58.0   \n",
       "would         8.0   6.0  ...   8.0    2.0     4.0   4.0       10.0      8.0   \n",
       "may           4.0  14.0  ...   6.0    4.0     6.0   4.0        8.0      4.0   \n",
       "probability   8.0   8.0  ...  20.0    4.0     2.0   0.0        6.0      8.0   \n",
       "fact          6.0  10.0  ...   0.0    2.0     4.0   0.0       16.0     22.0   \n",
       "number        4.0  10.0  ...  20.0    6.0     0.0  14.0        6.0      2.0   \n",
       "belief        8.0   0.0  ...   2.0    2.0    12.0   0.0       12.0      2.0   \n",
       "character     4.0   4.0  ...   8.0    8.0     0.0   0.0       12.0      6.0   \n",
       "true         20.0   8.0  ...   6.0   20.0     0.0   6.0        6.0      2.0   \n",
       "case          8.0   8.0  ...   0.0    0.0     2.0   4.0        6.0      6.0   \n",
       "inference     2.0   8.0  ...  12.0    2.0     0.0   0.0        8.0      4.0   \n",
       "rule          2.0  20.0  ...   6.0    0.0     0.0   0.0        2.0      2.0   \n",
       "first         2.0   4.0  ...  16.0    2.0     6.0   0.0        2.0      2.0   \n",
       "must          6.0   6.0  ...   2.0    2.0    10.0   2.0        4.0      2.0   \n",
       "certain       4.0  10.0  ...   2.0    8.0     0.0   2.0        0.0      2.0   \n",
       "two           6.0   0.0  ...   4.0    8.0     0.0   2.0        6.0      2.0   \n",
       "thing        20.0   0.0  ...   8.0    8.0     2.0   2.0        8.0      6.0   \n",
       "method        0.0   2.0  ...   0.0    2.0     8.0   2.0        6.0      2.0   \n",
       "time          6.0   4.0  ...   2.0    2.0     2.0   8.0        6.0      2.0   \n",
       "different     6.0   6.0  ...   6.0    8.0     6.0   6.0       20.0      4.0   \n",
       "another       2.0   6.0  ...   2.0    6.0     2.0   2.0        4.0      0.0   \n",
       "induction     8.0   2.0  ...   0.0    2.0     2.0   2.0        0.0      2.0   \n",
       "question      2.0   2.0  ...   2.0    2.0     0.0   2.0        0.0      2.0   \n",
       "man           2.0   2.0  ...   0.0    0.0     6.0   2.0        4.0     10.0   \n",
       "idea          2.0   2.0  ...   0.0    2.0     0.0   2.0        2.0      0.0   \n",
       "\n",
       "             induction  question   man  idea  \n",
       "one                4.0       8.0  10.0   6.0  \n",
       "would              2.0       8.0  10.0   0.0  \n",
       "may                8.0       8.0   6.0   4.0  \n",
       "probability        2.0       8.0   0.0  10.0  \n",
       "fact               6.0      14.0   2.0   6.0  \n",
       "number             4.0       2.0   0.0   2.0  \n",
       "belief             2.0       2.0   2.0   2.0  \n",
       "character          6.0       4.0   0.0   0.0  \n",
       "true               8.0       2.0   2.0   2.0  \n",
       "case               2.0       2.0   2.0   2.0  \n",
       "inference          8.0       0.0   6.0   0.0  \n",
       "rule               4.0       0.0   0.0   0.0  \n",
       "first              0.0      12.0   0.0   0.0  \n",
       "must               0.0       0.0   4.0   2.0  \n",
       "certain            2.0       2.0   4.0   0.0  \n",
       "two                0.0       2.0   0.0   0.0  \n",
       "thing              2.0       2.0   0.0   2.0  \n",
       "method             2.0       0.0   6.0   0.0  \n",
       "time               2.0       2.0   2.0   2.0  \n",
       "different          0.0       0.0   4.0   2.0  \n",
       "another            2.0       2.0  10.0   0.0  \n",
       "induction          4.0       0.0   0.0   0.0  \n",
       "question           0.0       4.0   0.0   2.0  \n",
       "man                0.0       0.0   0.0   2.0  \n",
       "idea               0.0       2.0   2.0   4.0  \n",
       "\n",
       "[25 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_corpus, small_vocab = prepare_corpus(corpus, 25)\n",
    "small_count_matrix, small_vocab_index = generate_count_matrix(prepared_corpus, small_vocab, 5)\n",
    "pd.DataFrame(small_count_matrix, index=small_vocab_index, columns=small_vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation\n",
    "\n",
    "Such a small matrix can be visualised using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x190f892b1c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(small_count_matrix, cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be converted to code to create a count matrix for the Brown corpus. It takes quite a while to run, however, and therefore I have disabled."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_text = list(brown.words(categories=\"news\"))\n",
    "brown_corpus = lemmatize_corpus(news_text)\n",
    "brown_vocab = list(set(brown_corpus).difference(stop_words))\n",
    "brown_count_matrix, brown_vocab_index = generate_count_matrix(brown_corpus, brown_vocab)\n",
    "pd.DataFrame(brown_count_matrix, index=brown_vocab_index, columns=brown_vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Scoring\n",
    "\n",
    "One of the core application of vector representations is to measure the similarity between concepts. We use cosine similarity between two vectors to calculate their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(first_vector: np.array, second_vector: np.array) -> float:\n",
    "    return 1 - distance.cosine(first_vector, second_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also create a function to extract word vectors from the count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word: str, count_matrix: np.matrix, vocab_index: dict) -> np.array:\n",
    "    i = vocab_index[word]\n",
    "    return np.array(count_matrix[i,:]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell I extract the vectors vectors for three words:\n",
    "- \"belief\"\n",
    "- \"horror\"\n",
    "- \"method\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_vector, h_vector, m_vector = map(lambda w: get_vector(w, count_matrix, vocab_index), (\"belief\", \"horror\", \"method\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, our vectors have the length of the entire vocabulary (excluding stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3857,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "\n",
    "Cosine similarity lies in the range $[-1, 1]$. A vector should have cosine similarity of 1 to itself and -1 to itself multiplied by -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(b_vector, b_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(b_vector, -1*b_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity between \"belief\" and \"horror\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015891043154093176"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(b_vector, h_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is a symmetric function, i.e. if we swap the two vectors we get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015891043154093176"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(h_vector, b_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, \"belief\" and \"method\" are more closely related than \"belief\" and \"horror\". With cosine similarity we can test this intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36151266252562264"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(b_vector, m_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10048348388226758"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(h_vector, m_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the matrix is sparse, the cosine similarity often becomes 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_vector = get_vector(\"german\", count_matrix, vocab_index)\n",
    "cosine_similarity(h_vector, g_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Pointwise Mutual Information\n",
    "\n",
    "Mere counts lead to extremely sparse matrices and do not account for the differences in frequency between words at all. A relatively simple approach to address these issues is to use *Pointwise Mutual Information* (PMI). For the occurence of two words in the same window, PMI is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{log}(\\frac{\\text{P}(A,B)}{\\text{P}(A)*\\text{P}(B)})\n",
    "\\end{equation}\n",
    "\n",
    "We estimate the probabilities using frequency counts from the count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi_calculation(count_matrix: np.matrix, smoothing: float = 0.01) -> np.matrix:\n",
    "    count_array = np.array(count_matrix) + smoothing\n",
    "    \n",
    "    num_ngrams = count_array.sum() / 2\n",
    "    all_sums = np.sum(count_array, axis=0)\n",
    "    \n",
    "    pmi_array = np.zeros_like(count_array)\n",
    "    \n",
    "    for i1, i2 in zip(*np.triu_indices_from(count_array)):\n",
    "        p_ab = count_array[i1, i2] / num_ngrams\n",
    "        p_a = all_sums[i1] / num_ngrams\n",
    "        p_b = all_sums[i2] / num_ngrams\n",
    "                \n",
    "        pmi_array[i1, i2] = np.log(p_ab/(p_a*p_b))\n",
    "        \n",
    "    return np.matrix(np.where(pmi_array,pmi_array,pmi_array.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mentioned</th>\n",
       "      <th>impossibility</th>\n",
       "      <th>upon</th>\n",
       "      <th>inductively</th>\n",
       "      <th>untraveled</th>\n",
       "      <th>intent</th>\n",
       "      <th>ask</th>\n",
       "      <th>affirm</th>\n",
       "      <th>unfamiliar</th>\n",
       "      <th>un-b-ness</th>\n",
       "      <th>...</th>\n",
       "      <th>evolution</th>\n",
       "      <th>unrelated</th>\n",
       "      <th>wavering</th>\n",
       "      <th>denied</th>\n",
       "      <th>similarity</th>\n",
       "      <th>candour</th>\n",
       "      <th>refined</th>\n",
       "      <th>lightning</th>\n",
       "      <th>limited</th>\n",
       "      <th>exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mentioned</th>\n",
       "      <td>-0.393776</td>\n",
       "      <td>-0.720570</td>\n",
       "      <td>-3.066895</td>\n",
       "      <td>-0.588299</td>\n",
       "      <td>-0.393776</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-1.294006</td>\n",
       "      <td>-0.514965</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.552304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393776</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.393776</td>\n",
       "      <td>-0.435826</td>\n",
       "      <td>-0.623042</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.435826</td>\n",
       "      <td>-0.514965</td>\n",
       "      <td>-1.345050</td>\n",
       "      <td>-1.553876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impossibility</th>\n",
       "      <td>-0.720570</td>\n",
       "      <td>-1.047363</td>\n",
       "      <td>-3.393689</td>\n",
       "      <td>4.388213</td>\n",
       "      <td>-0.720570</td>\n",
       "      <td>-0.676674</td>\n",
       "      <td>-1.620799</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>-0.676674</td>\n",
       "      <td>-0.879097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.720570</td>\n",
       "      <td>-0.676674</td>\n",
       "      <td>-0.720570</td>\n",
       "      <td>-0.762619</td>\n",
       "      <td>-0.949836</td>\n",
       "      <td>-0.676674</td>\n",
       "      <td>-0.762619</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>-1.671844</td>\n",
       "      <td>-1.880669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upon</th>\n",
       "      <td>-3.066895</td>\n",
       "      <td>-3.393689</td>\n",
       "      <td>-5.740014</td>\n",
       "      <td>-3.261418</td>\n",
       "      <td>-3.066895</td>\n",
       "      <td>-3.023000</td>\n",
       "      <td>-3.967125</td>\n",
       "      <td>-3.188084</td>\n",
       "      <td>-3.023000</td>\n",
       "      <td>-3.225423</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.066895</td>\n",
       "      <td>-3.023000</td>\n",
       "      <td>-3.066895</td>\n",
       "      <td>-3.108945</td>\n",
       "      <td>-3.296161</td>\n",
       "      <td>-3.023000</td>\n",
       "      <td>-3.108945</td>\n",
       "      <td>2.115221</td>\n",
       "      <td>-4.018169</td>\n",
       "      <td>-4.226995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inductively</th>\n",
       "      <td>-0.588299</td>\n",
       "      <td>4.388213</td>\n",
       "      <td>-3.261418</td>\n",
       "      <td>-0.782821</td>\n",
       "      <td>-0.588299</td>\n",
       "      <td>-0.544403</td>\n",
       "      <td>-1.488528</td>\n",
       "      <td>-0.709488</td>\n",
       "      <td>-0.544403</td>\n",
       "      <td>-0.746826</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.588299</td>\n",
       "      <td>-0.544403</td>\n",
       "      <td>-0.588299</td>\n",
       "      <td>-0.630348</td>\n",
       "      <td>-0.817565</td>\n",
       "      <td>-0.544403</td>\n",
       "      <td>-0.630348</td>\n",
       "      <td>-0.709488</td>\n",
       "      <td>-1.539573</td>\n",
       "      <td>-1.748398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>untraveled</th>\n",
       "      <td>-0.393776</td>\n",
       "      <td>-0.720570</td>\n",
       "      <td>-3.066895</td>\n",
       "      <td>-0.588299</td>\n",
       "      <td>-0.393776</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-1.294006</td>\n",
       "      <td>-0.514965</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.552304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393776</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.393776</td>\n",
       "      <td>-0.435826</td>\n",
       "      <td>-0.623042</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.435826</td>\n",
       "      <td>-0.514965</td>\n",
       "      <td>-1.345050</td>\n",
       "      <td>-1.553876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candour</th>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.676674</td>\n",
       "      <td>-3.023000</td>\n",
       "      <td>-0.544403</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.305985</td>\n",
       "      <td>-1.250110</td>\n",
       "      <td>-0.471070</td>\n",
       "      <td>-0.305985</td>\n",
       "      <td>-0.508409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.305985</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>-0.391930</td>\n",
       "      <td>-0.579147</td>\n",
       "      <td>-0.305985</td>\n",
       "      <td>-0.391930</td>\n",
       "      <td>-0.471070</td>\n",
       "      <td>-1.301155</td>\n",
       "      <td>-1.509981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refined</th>\n",
       "      <td>-0.435826</td>\n",
       "      <td>-0.762619</td>\n",
       "      <td>-3.108945</td>\n",
       "      <td>-0.630348</td>\n",
       "      <td>-0.435826</td>\n",
       "      <td>-0.391930</td>\n",
       "      <td>-1.336055</td>\n",
       "      <td>-0.557015</td>\n",
       "      <td>-0.391930</td>\n",
       "      <td>-0.594354</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435826</td>\n",
       "      <td>-0.391930</td>\n",
       "      <td>-0.435826</td>\n",
       "      <td>-0.477875</td>\n",
       "      <td>-0.665092</td>\n",
       "      <td>-0.391930</td>\n",
       "      <td>-0.477875</td>\n",
       "      <td>-0.557015</td>\n",
       "      <td>-1.387100</td>\n",
       "      <td>-1.595926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightning</th>\n",
       "      <td>-0.514965</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>2.115221</td>\n",
       "      <td>-0.709488</td>\n",
       "      <td>-0.514965</td>\n",
       "      <td>-0.471070</td>\n",
       "      <td>-1.415195</td>\n",
       "      <td>-0.636155</td>\n",
       "      <td>-0.471070</td>\n",
       "      <td>-0.673493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.514965</td>\n",
       "      <td>-0.471070</td>\n",
       "      <td>-0.514965</td>\n",
       "      <td>-0.557015</td>\n",
       "      <td>-0.744232</td>\n",
       "      <td>-0.471070</td>\n",
       "      <td>-0.557015</td>\n",
       "      <td>-0.636155</td>\n",
       "      <td>-1.466239</td>\n",
       "      <td>-1.675065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>limited</th>\n",
       "      <td>-1.345050</td>\n",
       "      <td>-1.671844</td>\n",
       "      <td>-4.018169</td>\n",
       "      <td>-1.539573</td>\n",
       "      <td>-1.345050</td>\n",
       "      <td>-1.301155</td>\n",
       "      <td>-2.245280</td>\n",
       "      <td>-1.466239</td>\n",
       "      <td>-1.301155</td>\n",
       "      <td>-1.503578</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.345050</td>\n",
       "      <td>-1.301155</td>\n",
       "      <td>-1.345050</td>\n",
       "      <td>-1.387100</td>\n",
       "      <td>-1.574316</td>\n",
       "      <td>-1.301155</td>\n",
       "      <td>-1.387100</td>\n",
       "      <td>-1.466239</td>\n",
       "      <td>4.389537</td>\n",
       "      <td>-2.505150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exist</th>\n",
       "      <td>-1.553876</td>\n",
       "      <td>-1.880669</td>\n",
       "      <td>-4.226995</td>\n",
       "      <td>-1.748398</td>\n",
       "      <td>-1.553876</td>\n",
       "      <td>-1.509981</td>\n",
       "      <td>-2.454106</td>\n",
       "      <td>-1.675065</td>\n",
       "      <td>-1.509981</td>\n",
       "      <td>-1.712404</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.553876</td>\n",
       "      <td>-1.509981</td>\n",
       "      <td>-1.553876</td>\n",
       "      <td>-1.595926</td>\n",
       "      <td>-1.783142</td>\n",
       "      <td>-1.509981</td>\n",
       "      <td>-1.595926</td>\n",
       "      <td>-1.675065</td>\n",
       "      <td>-2.505150</td>\n",
       "      <td>-2.713976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3857 rows × 3857 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               mentioned  impossibility      upon  inductively  untraveled  \\\n",
       "mentioned      -0.393776      -0.720570 -3.066895    -0.588299   -0.393776   \n",
       "impossibility  -0.720570      -1.047363 -3.393689     4.388213   -0.720570   \n",
       "upon           -3.066895      -3.393689 -5.740014    -3.261418   -3.066895   \n",
       "inductively    -0.588299       4.388213 -3.261418    -0.782821   -0.588299   \n",
       "untraveled     -0.393776      -0.720570 -3.066895    -0.588299   -0.393776   \n",
       "...                  ...            ...       ...          ...         ...   \n",
       "candour        -0.349881      -0.676674 -3.023000    -0.544403   -0.349881   \n",
       "refined        -0.435826      -0.762619 -3.108945    -0.630348   -0.435826   \n",
       "lightning      -0.514965      -0.841759  2.115221    -0.709488   -0.514965   \n",
       "limited        -1.345050      -1.671844 -4.018169    -1.539573   -1.345050   \n",
       "exist          -1.553876      -1.880669 -4.226995    -1.748398   -1.553876   \n",
       "\n",
       "                 intent       ask    affirm  unfamiliar  un-b-ness  ...  \\\n",
       "mentioned     -0.349881 -1.294006 -0.514965   -0.349881  -0.552304  ...   \n",
       "impossibility -0.676674 -1.620799 -0.841759   -0.676674  -0.879097  ...   \n",
       "upon          -3.023000 -3.967125 -3.188084   -3.023000  -3.225423  ...   \n",
       "inductively   -0.544403 -1.488528 -0.709488   -0.544403  -0.746826  ...   \n",
       "untraveled    -0.349881 -1.294006 -0.514965   -0.349881  -0.552304  ...   \n",
       "...                 ...       ...       ...         ...        ...  ...   \n",
       "candour       -0.305985 -1.250110 -0.471070   -0.305985  -0.508409  ...   \n",
       "refined       -0.391930 -1.336055 -0.557015   -0.391930  -0.594354  ...   \n",
       "lightning     -0.471070 -1.415195 -0.636155   -0.471070  -0.673493  ...   \n",
       "limited       -1.301155 -2.245280 -1.466239   -1.301155  -1.503578  ...   \n",
       "exist         -1.509981 -2.454106 -1.675065   -1.509981  -1.712404  ...   \n",
       "\n",
       "               evolution  unrelated  wavering    denied  similarity   candour  \\\n",
       "mentioned      -0.393776  -0.349881 -0.393776 -0.435826   -0.623042 -0.349881   \n",
       "impossibility  -0.720570  -0.676674 -0.720570 -0.762619   -0.949836 -0.676674   \n",
       "upon           -3.066895  -3.023000 -3.066895 -3.108945   -3.296161 -3.023000   \n",
       "inductively    -0.588299  -0.544403 -0.588299 -0.630348   -0.817565 -0.544403   \n",
       "untraveled     -0.393776  -0.349881 -0.393776 -0.435826   -0.623042 -0.349881   \n",
       "...                  ...        ...       ...       ...         ...       ...   \n",
       "candour        -0.349881  -0.305985 -0.349881 -0.391930   -0.579147 -0.305985   \n",
       "refined        -0.435826  -0.391930 -0.435826 -0.477875   -0.665092 -0.391930   \n",
       "lightning      -0.514965  -0.471070 -0.514965 -0.557015   -0.744232 -0.471070   \n",
       "limited        -1.345050  -1.301155 -1.345050 -1.387100   -1.574316 -1.301155   \n",
       "exist          -1.553876  -1.509981 -1.553876 -1.595926   -1.783142 -1.509981   \n",
       "\n",
       "                refined  lightning   limited     exist  \n",
       "mentioned     -0.435826  -0.514965 -1.345050 -1.553876  \n",
       "impossibility -0.762619  -0.841759 -1.671844 -1.880669  \n",
       "upon          -3.108945   2.115221 -4.018169 -4.226995  \n",
       "inductively   -0.630348  -0.709488 -1.539573 -1.748398  \n",
       "untraveled    -0.435826  -0.514965 -1.345050 -1.553876  \n",
       "...                 ...        ...       ...       ...  \n",
       "candour       -0.391930  -0.471070 -1.301155 -1.509981  \n",
       "refined       -0.477875  -0.557015 -1.387100 -1.595926  \n",
       "lightning     -0.557015  -0.636155 -1.466239 -1.675065  \n",
       "limited       -1.387100  -1.466239  4.389537 -2.505150  \n",
       "exist         -1.595926  -1.675065 -2.505150 -2.713976  \n",
       "\n",
       "[3857 rows x 3857 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_matrix = pmi_calculation(count_matrix)\n",
    "pd.DataFrame(p_matrix, index=vocab_index, columns=vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the cosine similarity for the vectors in the matrix after applying PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7134002139555036"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_vector, h_vector, m_vector = map(lambda w: get_vector(w, p_matrix, vocab_index), (\"belief\", \"horror\", \"method\"))\n",
    "cosine_similarity(b_vector, h_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8776176723516733"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(b_vector, m_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7484418637957632"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(h_vector, m_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Problem\n",
    "`log(0)` is undefined and to avoid it I use smoothing. The problem is that adding a count to every dimension quickly overwhelms the information in the vectors. We don't have that many occurrences in the first place. I had to use quite a small smoothing factor to get reasonable results. However, you will still finde that \"horror\" and \"german\" lie very close together. While this might tempt us to derive rash conclusions about what it means to be german, it is more likely an artefact resulting from extremely sparse word vectors which are then dominated by smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9564096949398055"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_vector = get_vector(\"german\", p_matrix, vocab_index)\n",
    "cosine_similarity(h_vector, g_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "So far, the vectors all had the dimensionality of the entire vocabulary. For our small corpus, that was still workable but with a larger vocabulary the dimensionality becomes excessive. To reduce the dimensionality, Singular Value Decomposition is frequently used. I reduce the word vectors to 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_reduction(matrix: np.matrix, n_components: int = 100) -> np.matrix:\n",
    "    u, s, vh = svd(matrix,\n",
    "                   full_matrices=False,\n",
    "                   compute_uv=True)\n",
    "    \n",
    "    u = np.array(u) # multiplication will fail if u is a matrix, because the axes are not appropriately reduced\n",
    "\n",
    "    reduced_u = u[:, :n_components]\n",
    "    reduced_s = s[:n_components]\n",
    "    \n",
    "    return np.matrix(reduced_u * reduced_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_matrix = svd_reduction(p_matrix)\n",
    "reduced_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(reduced_matrix, index=vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the resulting word vectors still exhibit comparable cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_vector, h_vector, m_vector = map(lambda w: get_vector(w, reduced_matrix, vocab_index), (\"belief\", \"horror\", \"method\"))\n",
    "cosine_similarity(b_vector, h_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(b_vector, m_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(h_vector, m_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Comparison\n",
    "\n",
    "Having tried out some old-school techniques, how does Word2Vec compare? A quick comparison suggets its benefits:\n",
    "- Existing libraries make creation and use very ease\n",
    "- The resulting vectors fit our linguistic intuitions, in this case that \"belief\" and \"method\" are closer than e.g. than \"belief\" and \"horror\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = nltk.sent_tokenize(corpus)\n",
    "sent_tokenized_text = [nltk.word_tokenize(sent) for sent in sent_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec(sentences=sent_tokenized_text, size=100, window=10, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wv_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_wv_vector, h_wv_vector, m_wv_vector = map(lambda w: wv_model.wv[w], (\"belief\", \"horror\", \"method\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(b_wv_vector, b_wv_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(b_wv_vector, h_wv_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(b_wv_vector, m_wv_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(h_wv_vector, m_wv_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
